{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# !/usr/env python3\n",
    "\n",
    "'''\n",
    "Output:\n",
    "video captioning model itself and produce loss curve\n",
    "\n",
    "Usage:\n",
    "main document to train the video captioning model\n",
    "'''\n",
    "\n",
    "# set up\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "from util import *\n",
    "from model.video_caption import sequence_2_sequence_LSTM\n",
    "from load_caption_feature import *\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import gc\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process and save data  \n",
    "Load caption Xtrain, Xtest, ytrain, ytest, video_train, video_test and save them.  \n",
    "Here in details, see **load_caption_feature.py**. Only need to run once, codes are going to save the data file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load and save training data\n",
    "num_frames = 15\n",
    "size = (224, 224, 3)\n",
    "\n",
    "# more balanced data\n",
    "idx_path = os.getcwd() + '/datasets/x_train_ind_above400.npy'\n",
    "Xtrain_idx = np.load(idx_path)\n",
    "labels = np.load(os.getcwd() + '/datasets/y_train_mapped_above400.npy')\n",
    "\n",
    "# if all videos then \n",
    "num_videos = len(Xtrain_idx)\n",
    "\n",
    "tic = datetime.now()\n",
    "# for clearing memory convenience\n",
    "model = vgg_16_pretrained()\n",
    "Xtr, ytr = load_features(model, num_videos, num_frames, Xtrain_idx, labels, size = (224, 224, 3), train_test_flag='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# clear memory\n",
    "del Xtr\n",
    "del ytr\n",
    "model = None\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load and save test data\n",
    "num_frames_test = 15\n",
    "size = (224, 224, 3)\n",
    "\n",
    "# more balanced data\n",
    "idx_path = os.getcwd() + '/datasets/x_test_ind_above400.npy'\n",
    "Xtest_idx = np.load(idx_path)\n",
    "ytest = np.load(os.getcwd() + '/datasets/y_test_mapped_above400.npy')\n",
    "\n",
    "# if all videos then \n",
    "num_videos_test = len(Xtest_idx)\n",
    "\n",
    "model = vgg_16_pretrained()\n",
    "Xte, yte = load_features(model, num_videos_test, num_frames_test, Xtest_idx, ytest, size = (224, 224, 3), train_test_flag='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# clean memory\n",
    "del Xte\n",
    "del yte\n",
    "model = None\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save input frames train and test\n",
    "curr = os.getcwd() + '/datasets'\n",
    "def save_frames(X, vid_ls, mode = 'train'):\n",
    "    X = X.reshape((-1, 15, 4096))\n",
    "    assert X.shape[0] == len(vid_ls)\n",
    "    input_frames = {}\n",
    "    for i in range(X.shape[0]):\n",
    "        vid = vid_ls[i]\n",
    "        input_frames[vid] = Xtr[i]\n",
    "    pickle.dump(input_frames, open(curr + '/input_frames_' + mode + '.pickle', 'wb'))\n",
    "\n",
    "vid_train = np.load(curr + '/videoIdtrain_allCap_15frames.npy')\n",
    "Xtr = np.load(curr + '/Xtrain_allCap_15frames.npy')\n",
    "Xte = np.load(curr + '/Xtest_allCap_15frames.npy')\n",
    "vid_test = np.load(curr + '/videoIdtest_allCap_15frames.npy')\n",
    "save_frames(Xtr, vid_train, mode = 'train')\n",
    "save_frames(Xte, vid_test, mode = 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model setup and train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Configuration and Parameters \n",
    "Parameters:  \n",
    "\n",
    "* model_name: the name of model, here we refer sequence to sequence model from [1](https://arxiv.org/abs/1505.00487);  \n",
    "* state_size: lstm encoder and encoder state dimension  \n",
    "* learning_rate: learning rate  \n",
    "* input_size: vector size input to lstm, here we use pretrained VGG16 output 4096 dimension  \n",
    "* batch_size: batch size  \n",
    "* max_sentence_length: fixed length for captions, default is 20 \n",
    "* word_vector_size: depends on vocabulary chosen, here is 50, but can be changed \n",
    "* voc_size: depends on vocabulary created, if self-created vocabulary, it is 6169, if glove\n",
    "* n_epoches: the number of epoches to run  \n",
    "* num_frames: frame number  \n",
    "* hidden_size: lstm encoder and encoder hidden dimension \n",
    "\n",
    "**Reference**  \n",
    "[1] Venugopalan, S., Rohrbach, M., Donahue, J., Mooney, R., Darrell, T., & Saenko, K. (2015). Sequence to sequence-video to text. In Proceedings of the IEEE International Conference on Computer Vision (pp. 4534-4542).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define parameters\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "#=======Change These===============================\n",
    "tf.app.flags.DEFINE_string(\"model_name\", \"sequence2sequence\", \"name of the model\")\n",
    "tf.app.flags.DEFINE_integer(\"state_size\", 100, \"Size of each model layer.\")\n",
    "#==================================================\n",
    "\n",
    "tf.app.flags.DEFINE_float(\"input_size\", 4096, \"input size for each frame\")\n",
    "tf.app.flags.DEFINE_integer(\"max_sentence_length\", 20, \"maximum captioning sentence length\")\n",
    "tf.app.flags.DEFINE_integer(\"word_vector_size\", 50, \"word embedding dimension default is 25 for twitter glove\")\n",
    "tf.app.flags.DEFINE_integer(\"num_frames\", 15, \"number of frames per video\")\n",
    "FLAGS = tf.app.flags.FLAGS        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish loading training data!\n"
     ]
    }
   ],
   "source": [
    "def get_session():\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    session = tf.Session(config=config)\n",
    "    return session\n",
    "\n",
    "curPath = os.getcwd()\n",
    "dataPath = curPath + \"/datasets/\"\n",
    "\n",
    "# pick first 100 for debugging purpose\n",
    "# load data\n",
    "sample_size = 500\n",
    "wvector_dim = 50\n",
    "is_training = True\n",
    "input_frames_train, captions_train, \\\n",
    "        word_dict, word2Index, index2Word = load_caption_data(sample_size, dataPath, train = is_training)\n",
    "word_embedding = word_embedding_array(word_dict, wvector_dim, word2Index) \n",
    "print('Finish loading training data!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4270\n",
      "4270\n"
     ]
    }
   ],
   "source": [
    "print( len(input_frames_train.keys()) )\n",
    "print( len(captions_train.keys()) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Model Graph  \n",
    "In details, see **model/video_caption.py**.   \n",
    "We refer to sequence to sequence model to build two LSTM layers. One is encoder, and the other is decoder. The cell number of decoder depends on the maximum caption length we set. Here we choose 20 here by exploring the distribution of captions. In order to make encoder have same length of outputs, we add pad cells to encoder. Encoder accept 15 frames VGG16 outputs so that it has 15 cells. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start building model ...\n",
      "total number of parameter 2403469\n"
     ]
    }
   ],
   "source": [
    "# build model graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "lr = 1e-4\n",
    "hidden_size = 100\n",
    "state_size = 100\n",
    "batch_size = 64\n",
    "voc_size = 6169\n",
    "epoch = 50\n",
    "\n",
    "model = sequence_2_sequence_LSTM(word_embedding, FLAGS, batch_size=batch_size, hidden_size=hidden_size,\n",
    "        voc_size = voc_size, n_epochs = epoch, lr = lr, reg = 1e-4, mode = 'train', save_model_file = 'bestModel')\n",
    "model.train_embedding = False\n",
    "model.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'LSTM_seq2seq/encoder/layer_norm_basic_lstm_cell/weights:0' shape=(4196, 400) dtype=float32_ref>\n",
      "<tf.Variable 'LSTM_seq2seq/encoder/layer_norm_basic_lstm_cell/input/gamma:0' shape=(100,) dtype=float32_ref>\n",
      "<tf.Variable 'LSTM_seq2seq/encoder/layer_norm_basic_lstm_cell/input/beta:0' shape=(100,) dtype=float32_ref>\n",
      "<tf.Variable 'LSTM_seq2seq/encoder/layer_norm_basic_lstm_cell/transform/gamma:0' shape=(100,) dtype=float32_ref>\n",
      "<tf.Variable 'LSTM_seq2seq/encoder/layer_norm_basic_lstm_cell/transform/beta:0' shape=(100,) dtype=float32_ref>\n",
      "<tf.Variable 'LSTM_seq2seq/encoder/layer_norm_basic_lstm_cell/forget/gamma:0' shape=(100,) dtype=float32_ref>\n",
      "<tf.Variable 'LSTM_seq2seq/encoder/layer_norm_basic_lstm_cell/forget/beta:0' shape=(100,) dtype=float32_ref>\n",
      "<tf.Variable 'LSTM_seq2seq/encoder/layer_norm_basic_lstm_cell/output/gamma:0' shape=(100,) dtype=float32_ref>\n",
      "<tf.Variable 'LSTM_seq2seq/encoder/layer_norm_basic_lstm_cell/output/beta:0' shape=(100,) dtype=float32_ref>\n",
      "<tf.Variable 'LSTM_seq2seq/encoder/layer_norm_basic_lstm_cell/state/gamma:0' shape=(100,) dtype=float32_ref>\n",
      "<tf.Variable 'LSTM_seq2seq/encoder/layer_norm_basic_lstm_cell/state/beta:0' shape=(100,) dtype=float32_ref>\n",
      "<tf.Variable 'LSTM_seq2seq/decoder/layer_norm_basic_lstm_cell/weights:0' shape=(250, 400) dtype=float32_ref>\n",
      "<tf.Variable 'LSTM_seq2seq/decoder/layer_norm_basic_lstm_cell/input/gamma:0' shape=(100,) dtype=float32_ref>\n",
      "<tf.Variable 'LSTM_seq2seq/decoder/layer_norm_basic_lstm_cell/input/beta:0' shape=(100,) dtype=float32_ref>\n",
      "<tf.Variable 'LSTM_seq2seq/decoder/layer_norm_basic_lstm_cell/transform/gamma:0' shape=(100,) dtype=float32_ref>\n",
      "<tf.Variable 'LSTM_seq2seq/decoder/layer_norm_basic_lstm_cell/transform/beta:0' shape=(100,) dtype=float32_ref>\n",
      "<tf.Variable 'LSTM_seq2seq/decoder/layer_norm_basic_lstm_cell/forget/gamma:0' shape=(100,) dtype=float32_ref>\n",
      "<tf.Variable 'LSTM_seq2seq/decoder/layer_norm_basic_lstm_cell/forget/beta:0' shape=(100,) dtype=float32_ref>\n",
      "<tf.Variable 'LSTM_seq2seq/decoder/layer_norm_basic_lstm_cell/output/gamma:0' shape=(100,) dtype=float32_ref>\n",
      "<tf.Variable 'LSTM_seq2seq/decoder/layer_norm_basic_lstm_cell/output/beta:0' shape=(100,) dtype=float32_ref>\n",
      "<tf.Variable 'LSTM_seq2seq/decoder/layer_norm_basic_lstm_cell/state/gamma:0' shape=(100,) dtype=float32_ref>\n",
      "<tf.Variable 'LSTM_seq2seq/decoder/layer_norm_basic_lstm_cell/state/beta:0' shape=(100,) dtype=float32_ref>\n",
      "<tf.Variable 'LSTM_seq2seq/decoder/hidden_to_scores/kernel:0' shape=(100, 6169) dtype=float32_ref>\n",
      "<tf.Variable 'LSTM_seq2seq/decoder/hidden_to_scores/bias:0' shape=(6169,) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "# check variables\n",
    "for v in tf.trainable_variables():\n",
    "    print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[64,35,4096]\n\t [[Node: LSTM_seq2seq/encoder/concat = ConcatV2[N=2, T=DT_FLOAT, Tidx=DT_INT32, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](_recv_Placeholder_0/_91, LSTM_seq2seq/encoder/zeros, LSTM_seq2seq/encoder/concat/axis)]]\n\t [[Node: LSTM_seq2seq/decoder/truediv/_255 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_78057_LSTM_seq2seq/decoder/truediv\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n\nCaused by op 'LSTM_seq2seq/encoder/concat', defined at:\n  File \"/usr/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/sunjiajun/cs231n_project/.env/lib/python3.5/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/home/sunjiajun/cs231n_project/.env/lib/python3.5/site-packages/traitlets/config/application.py\", line 592, in launch_instance\n    app.start()\n  File \"/home/sunjiajun/cs231n_project/.env/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 405, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/sunjiajun/cs231n_project/.env/lib/python3.5/site-packages/zmq/eventloop/ioloop.py\", line 162, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/sunjiajun/cs231n_project/.env/lib/python3.5/site-packages/tornado/ioloop.py\", line 883, in start\n    handler_func(fd_obj, events)\n  File \"/home/sunjiajun/cs231n_project/.env/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/sunjiajun/cs231n_project/.env/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/sunjiajun/cs231n_project/.env/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/sunjiajun/cs231n_project/.env/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/sunjiajun/cs231n_project/.env/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/sunjiajun/cs231n_project/.env/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 260, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/sunjiajun/cs231n_project/.env/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 212, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/sunjiajun/cs231n_project/.env/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 370, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/sunjiajun/cs231n_project/.env/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 175, in do_execute\n    shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/sunjiajun/cs231n_project/.env/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2902, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/sunjiajun/cs231n_project/.env/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 3012, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/sunjiajun/cs231n_project/.env/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 3066, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-5-c4ad39a26316>\", line 14, in <module>\n    model.build()\n  File \"/home/sunjiajun/cs231n_project/model/video_caption.py\", line 111, in build\n    self.pred = self.add_prediction_op()\n  File \"/home/sunjiajun/cs231n_project/model/video_caption.py\", line 191, in add_prediction_op\n    encoder_output, encoder_state = self.encoder()\n  File \"/home/sunjiajun/cs231n_project/model/video_caption.py\", line 399, in encoder\n    enc_inp = tf.concat([input_batch, pads], axis = 1)\n  File \"/home/sunjiajun/cs231n_project/.env/lib/python3.5/site-packages/tensorflow/python/ops/array_ops.py\", line 1034, in concat\n    name=name)\n  File \"/home/sunjiajun/cs231n_project/.env/lib/python3.5/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 519, in _concat_v2\n    name=name)\n  File \"/home/sunjiajun/cs231n_project/.env/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 768, in apply_op\n    op_def=op_def)\n  File \"/home/sunjiajun/cs231n_project/.env/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2336, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/sunjiajun/cs231n_project/.env/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1228, in __init__\n    self._traceback = _extract_stack()\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[64,35,4096]\n\t [[Node: LSTM_seq2seq/encoder/concat = ConcatV2[N=2, T=DT_FLOAT, Tidx=DT_INT32, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](_recv_Placeholder_0/_91, LSTM_seq2seq/encoder/zeros, LSTM_seq2seq/encoder/concat/axis)]]\n\t [[Node: LSTM_seq2seq/decoder/truediv/_255 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_78057_LSTM_seq2seq/decoder/truediv\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m/home/sunjiajun/cs231n_project/.env/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1038\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1040\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/sunjiajun/cs231n_project/.env/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1021\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1022\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python3.5/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type, value, traceback)\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m                 \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/sunjiajun/cs231n_project/.env/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[1;34m()\u001b[0m\n\u001b[0;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 466\u001b[1;33m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[0;32m    467\u001b[0m   \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[64,35,4096]\n\t [[Node: LSTM_seq2seq/encoder/concat = ConcatV2[N=2, T=DT_FLOAT, Tidx=DT_INT32, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](_recv_Placeholder_0/_91, LSTM_seq2seq/encoder/zeros, LSTM_seq2seq/encoder/concat/axis)]]\n\t [[Node: LSTM_seq2seq/decoder/truediv/_255 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_78057_LSTM_seq2seq/decoder/truediv\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-65a677805f84>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0minput_frames_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptions_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/home/sunjiajun/cs231n_project/model/video_caption.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, sess, train_data, verbose)\u001b[0m\n\u001b[0;32m    323\u001b[0m         \u001b[0mprog\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mProgbar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 325\u001b[1;33m             \u001b[0mdev_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mavg_train_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    326\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    327\u001b[0m                 \u001b[1;31m# print epoch results\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/sunjiajun/cs231n_project/model/video_caption.py\u001b[0m in \u001b[0;36mrun_epoch\u001b[1;34m(self, sess, train_data, valid_data, verbose)\u001b[0m\n\u001b[0;32m    303\u001b[0m             \u001b[0mvid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcap\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvid\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 305\u001b[1;33m             \u001b[0mtrain_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcap\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    306\u001b[0m             \u001b[0mprog\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexact\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"train loss\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    307\u001b[0m             \u001b[0mtrain_losses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/sunjiajun/cs231n_project/model/video_caption.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, sess, input_frames, input_caption)\u001b[0m\n\u001b[0;32m    253\u001b[0m                                      \u001b[0minput_caption\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_caption\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m                                      is_training=True)\n\u001b[1;32m--> 255\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredict_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_ind\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredict_index\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/sunjiajun/cs231n_project/.env/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    776\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 778\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    779\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/sunjiajun/cs231n_project/.env/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    980\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 982\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    983\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/sunjiajun/cs231n_project/.env/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1030\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1032\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1033\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32m/home/sunjiajun/cs231n_project/.env/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1050\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1051\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1052\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1053\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1054\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[64,35,4096]\n\t [[Node: LSTM_seq2seq/encoder/concat = ConcatV2[N=2, T=DT_FLOAT, Tidx=DT_INT32, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](_recv_Placeholder_0/_91, LSTM_seq2seq/encoder/zeros, LSTM_seq2seq/encoder/concat/axis)]]\n\t [[Node: LSTM_seq2seq/decoder/truediv/_255 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_78057_LSTM_seq2seq/decoder/truediv\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n\nCaused by op 'LSTM_seq2seq/encoder/concat', defined at:\n  File \"/usr/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/sunjiajun/cs231n_project/.env/lib/python3.5/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/home/sunjiajun/cs231n_project/.env/lib/python3.5/site-packages/traitlets/config/application.py\", line 592, in launch_instance\n    app.start()\n  File \"/home/sunjiajun/cs231n_project/.env/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 405, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/sunjiajun/cs231n_project/.env/lib/python3.5/site-packages/zmq/eventloop/ioloop.py\", line 162, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/sunjiajun/cs231n_project/.env/lib/python3.5/site-packages/tornado/ioloop.py\", line 883, in start\n    handler_func(fd_obj, events)\n  File \"/home/sunjiajun/cs231n_project/.env/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/sunjiajun/cs231n_project/.env/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/sunjiajun/cs231n_project/.env/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/sunjiajun/cs231n_project/.env/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/sunjiajun/cs231n_project/.env/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/sunjiajun/cs231n_project/.env/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 260, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/sunjiajun/cs231n_project/.env/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 212, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/sunjiajun/cs231n_project/.env/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 370, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/sunjiajun/cs231n_project/.env/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 175, in do_execute\n    shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/sunjiajun/cs231n_project/.env/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2902, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/sunjiajun/cs231n_project/.env/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 3012, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/sunjiajun/cs231n_project/.env/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 3066, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-5-c4ad39a26316>\", line 14, in <module>\n    model.build()\n  File \"/home/sunjiajun/cs231n_project/model/video_caption.py\", line 111, in build\n    self.pred = self.add_prediction_op()\n  File \"/home/sunjiajun/cs231n_project/model/video_caption.py\", line 191, in add_prediction_op\n    encoder_output, encoder_state = self.encoder()\n  File \"/home/sunjiajun/cs231n_project/model/video_caption.py\", line 399, in encoder\n    enc_inp = tf.concat([input_batch, pads], axis = 1)\n  File \"/home/sunjiajun/cs231n_project/.env/lib/python3.5/site-packages/tensorflow/python/ops/array_ops.py\", line 1034, in concat\n    name=name)\n  File \"/home/sunjiajun/cs231n_project/.env/lib/python3.5/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 519, in _concat_v2\n    name=name)\n  File \"/home/sunjiajun/cs231n_project/.env/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 768, in apply_op\n    op_def=op_def)\n  File \"/home/sunjiajun/cs231n_project/.env/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2336, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/sunjiajun/cs231n_project/.env/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1228, in __init__\n    self._traceback = _extract_stack()\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[64,35,4096]\n\t [[Node: LSTM_seq2seq/encoder/concat = ConcatV2[N=2, T=DT_FLOAT, Tidx=DT_INT32, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](_recv_Placeholder_0/_91, LSTM_seq2seq/encoder/zeros, LSTM_seq2seq/encoder/concat/axis)]]\n\t [[Node: LSTM_seq2seq/decoder/truediv/_255 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_78057_LSTM_seq2seq/decoder/truediv\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n"
     ]
    }
   ],
   "source": [
    "# run training mode\n",
    "with get_session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    out = model.train(sess, (input_frames_train, captions_train), verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# unpack\n",
    "val_loss, tr_loss, tr_pred, val_pred, train_vid, val_vid = out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Learning Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot learning curve\n",
    "plt.plot(range(len(tr_loss)), tr_loss, 'r-', linewidth = 2, label = 'train')\n",
    "plt.plot(range(len(val_loss)), val_loss, 'b-', linewidth = 2, label = 'validation')\n",
    "plt.grid()\n",
    "plt.xlabel('iteration', fontsize = 13)\n",
    "plt.ylabel('loss', fontsize = 13)\n",
    "plt.title('iteration vs loss', fontsize = 15)\n",
    "plt.legend()\n",
    "plt.savefig(os.getcwd() + '/output/caption_learning_curve.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check batch captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# check captions\n",
    "sample_size = 100\n",
    "wvector_dim = 50\n",
    "is_training = False\n",
    "input_frames_test, captions_test = load_caption_data(sample_size, dataPath, train = is_training)\n",
    "model.mode = 'test'\n",
    "with get_session() as sess:\n",
    "    saver = tf.train.Saver()\n",
    "    # saver.restore(sess, os.getcwd() + \"/model/bestModel.ckpt\")\n",
    "    out = model.predict(sess, (input_frames_test, captions_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list_video_index, list_predict_index = out\n",
    "captions = {k: v for k, v in captions_test}\n",
    "# caption check\n",
    "for j in range(list_video_index):\n",
    "    words = []\n",
    "    trues = []\n",
    "    sample = list_predict_index[j]\n",
    "    vid = list_video_index[j]\n",
    "    cap = captions[vid]\n",
    "    for idx, i in enumerate(sample):\n",
    "        word = index2Word[i]\n",
    "        true = index2Word[cap[idx]]\n",
    "        if word not in ['<START>', '<END>', '<pad>']:\n",
    "            words.append(word)\n",
    "        if true not in ['<START>', '<END>', '<pad>']:\n",
    "            trues.append(true)   \n",
    "    print('Pred Caption: ', ' '.join(w for w in words))\n",
    "    print('True Caption: ', ' '.join(t for t in trues))\n",
    "    print('-'*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Test Prediction and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
