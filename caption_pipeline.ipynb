{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# !/usr/env python3\n",
    "\n",
    "'''\n",
    "Output:\n",
    "video captioning model itself and produce loss curve\n",
    "\n",
    "Usage:\n",
    "main document to train the video captioning model\n",
    "'''\n",
    "\n",
    "# set up\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "from util import *\n",
    "from model.video_caption import sequence_2_sequence_LSTM\n",
    "from model.image_caption import image_caption_LSTM\n",
    "\n",
    "from load_caption_feature import *\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import gc\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process and save data  \n",
    "Load caption Xtrain, Xtest, ytrain, ytest, video_train, video_test and save them.  \n",
    "Here in details, see **load_caption_feature.py**. Only need to run once, codes are going to save the data file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load and save training data\n",
    "num_frames = 15\n",
    "size = (224, 224, 3)\n",
    "\n",
    "# more balanced data\n",
    "idx_path = os.getcwd() + '/datasets/x_train_ind_above400.npy'\n",
    "Xtrain_idx = np.load(idx_path)\n",
    "labels = np.load(os.getcwd() + '/datasets/y_train_mapped_above400.npy')\n",
    "\n",
    "# if all videos then \n",
    "num_videos = len(Xtrain_idx)\n",
    "\n",
    "tic = datetime.now()\n",
    "# for clearing memory convenience\n",
    "model = vgg_16_pretrained()\n",
    "Xtr, ytr = load_features(model, num_videos, num_frames, Xtrain_idx, labels, size = (224, 224, 3), train_test_flag='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# clear memory\n",
    "del Xtr\n",
    "del ytr\n",
    "model = None\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load and save test data\n",
    "num_frames_test = 15\n",
    "size = (224, 224, 3)\n",
    "\n",
    "# more balanced data\n",
    "idx_path = os.getcwd() + '/datasets/x_test_ind_above400.npy'\n",
    "Xtest_idx = np.load(idx_path)\n",
    "ytest = np.load(os.getcwd() + '/datasets/y_test_mapped_above400.npy')\n",
    "\n",
    "# if all videos then \n",
    "num_videos_test = len(Xtest_idx)\n",
    "\n",
    "model = vgg_16_pretrained()\n",
    "Xte, yte = load_features(model, num_videos_test, num_frames_test, Xtest_idx, ytest, size = (224, 224, 3), train_test_flag='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# clean memory\n",
    "del Xte\n",
    "del yte\n",
    "model = None\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save input frames train and test\n",
    "curr = os.getcwd() + '/datasets'\n",
    "def save_frames(X, vid_ls, mode = 'train'):\n",
    "    X = X.reshape((-1, 15, 4096))\n",
    "    assert X.shape[0] == len(vid_ls)\n",
    "    input_frames = {}\n",
    "    for i in range(X.shape[0]):\n",
    "        vid = vid_ls[i]\n",
    "        input_frames[vid] = X[i]\n",
    "    pickle.dump(input_frames, open(curr + '/input_frames_' + mode + '.pickle', 'wb'))\n",
    "\n",
    "vid_train = np.load(curr + '/videoIdtrain_allCap_15frames.npy')\n",
    "Xtr = np.load(curr + '/Xtrain_allCap_15frames.npy')\n",
    "Xte = np.load(curr + '/Xtest_allCap_15frames.npy')\n",
    "vid_test = np.load(curr + '/videoIdtest_allCap_15frames.npy')\n",
    "save_frames(Xtr, vid_train, mode = 'train')\n",
    "save_frames(Xte, vid_test, mode = 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model setup and train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Configuration and Parameters \n",
    "Parameters:  \n",
    "\n",
    "* model_name: the name of model, here we refer sequence to sequence model from [1](https://arxiv.org/abs/1505.00487);  \n",
    "* state_size: lstm encoder and encoder state dimension  \n",
    "* learning_rate: learning rate  \n",
    "* input_size: vector size input to lstm, here we use pretrained VGG16 output 4096 dimension  \n",
    "* batch_size: batch size  \n",
    "* max_sentence_length: fixed length for captions, default is 20 \n",
    "* word_vector_size: depends on vocabulary chosen, here is 50, but can be changed \n",
    "* voc_size: depends on vocabulary created, if self-created vocabulary, it is 6169, if glove\n",
    "* n_epoches: the number of epoches to run  \n",
    "* num_frames: frame number  \n",
    "* hidden_size: lstm encoder and encoder hidden dimension \n",
    "\n",
    "**Reference**  \n",
    "[1] Venugopalan, S., Rohrbach, M., Donahue, J., Mooney, R., Darrell, T., & Saenko, K. (2015). Sequence to sequence-video to text. In Proceedings of the IEEE International Conference on Computer Vision (pp. 4534-4542).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define parameters\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "#=======Change These===============================\n",
    "tf.app.flags.DEFINE_string(\"model_name\", \"sequence2sequence\", \"name of the model\")\n",
    "tf.app.flags.DEFINE_integer(\"state_size\", 512, \"Size of each model layer.\")\n",
    "#==================================================\n",
    "\n",
    "tf.app.flags.DEFINE_float(\"input_size\", 4096, \"input size for each frame\")\n",
    "tf.app.flags.DEFINE_integer(\"max_sentence_length\", 20, \"maximum captioning sentence length\")\n",
    "tf.app.flags.DEFINE_integer(\"word_vector_size\", 50, \"word embedding dimension default is 25 for twitter glove\")\n",
    "tf.app.flags.DEFINE_integer(\"num_frames\", 15, \"number of frames per video\")\n",
    "FLAGS = tf.app.flags.FLAGS        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_session():\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    session = tf.Session(config=config)\n",
    "    return session\n",
    "\n",
    "curPath = os.getcwd()\n",
    "dataPath = curPath + \"/datasets/\"\n",
    "\n",
    "# pick first 100 for debugging purpose\n",
    "# load data\n",
    "sample_size = 500\n",
    "wvector_dim = 50\n",
    "is_training = True\n",
    "input_frames_train, captions_train, \\\n",
    "        word_dict, word2Index, index2Word = load_caption_data(sample_size, dataPath, train = is_training)\n",
    "word_embedding = word_embedding_array(word_dict, wvector_dim, word2Index) \n",
    "print('Finish loading training data!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Model Graph  \n",
    "In details, see **model/video_caption.py**.   \n",
    "We refer to sequence to sequence model to build two LSTM layers. One is encoder, and the other is decoder. The cell number of decoder depends on the maximum caption length we set. Here we choose 20 here by exploring the distribution of captions. In order to make encoder have same length of outputs, we add pad cells to encoder. Encoder accept 15 frames VGG16 outputs so that it has 15 cells. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# build model graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "lr = 1e-3\n",
    "hidden_size = 512\n",
    "state_size = 512\n",
    "batch_size = 64\n",
    "voc_size = 6169\n",
    "epoch = 7\n",
    "\n",
    "model = image_caption_LSTM(word_embedding, FLAGS, batch_size=batch_size, hidden_size=hidden_size,\n",
    "        voc_size = voc_size, n_epochs = epoch, lr = lr, reg = 1e-4, mode = 'train', save_model_file = 'bestModel')\n",
    "model.train_embedding = False\n",
    "model.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# check variables\n",
    "for v in tf.trainable_variables():\n",
    "    print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# run training mode\n",
    "with get_session() as sess:\n",
    "    saver = tf.train.Saver()\n",
    "#     saver.restore(sess, os.getcwd() + \"/model/bestModel.ckpt\")\n",
    "#     saver.restore(sess, os.getcwd() + \"/model/lastestModel.ckpt\")\n",
    "#     sess.run(tf.global_variables_initializer())\n",
    "    out = model.train(sess, (input_frames_train, captions_train), verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# unpack\n",
    "val_loss, tr_loss, tr_pred, val_pred, train_vid, val_vid = out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Learning Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot learning curve\n",
    "plt.plot(range(len(tr_loss)), tr_loss, 'r-', linewidth = 2, label = 'train')\n",
    "plt.plot(range(len(val_loss)), val_loss, 'b-', linewidth = 2, label = 'validation')\n",
    "plt.grid()\n",
    "plt.xlabel('iteration', fontsize = 13)\n",
    "plt.ylabel('loss', fontsize = 13)\n",
    "plt.title('iteration vs loss', fontsize = 15)\n",
    "plt.legend()\n",
    "plt.savefig(os.getcwd() + '/output/caption_learning_curve.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check batch captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# check captions\n",
    "sample_size = 100\n",
    "wvector_dim = 50\n",
    "is_training = False\n",
    "input_frames_test, captions_test = load_caption_data(sample_size, dataPath, train = is_training)\n",
    "model.mode = 'test'\n",
    "with get_session() as sess:\n",
    "    saver = tf.train.Saver()\n",
    "    # saver.restore(sess, os.getcwd() + \"/model/bestModel.ckpt\")\n",
    "    out = model.predict(sess, (input_frames_test, captions_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# list_video_index, list_predict_index = out\n",
    "# captions = {k: v for k, v in captions_test}\n",
    "captions = captions_train\n",
    "\n",
    "pred = val_pred\n",
    "captions_pred = {}\n",
    "captions_true = {}\n",
    "\n",
    "# caption check\n",
    "for j, vid in enumerate(val_vid):\n",
    "    words = []\n",
    "    trues = []\n",
    "    sample = val_pred[j]\n",
    "    cap = captions[vid]\n",
    "    for idx, i in enumerate(sample):\n",
    "        word = index2Word[i]\n",
    "        true = index2Word[cap[idx]]\n",
    "        if word not in ['<START>', '<END>', '<pad>']:\n",
    "            words.append(word)\n",
    "        if true not in ['<START>', '<END>', '<pad>']:\n",
    "            trues.append(true)   \n",
    "            \n",
    "    captions_pred[vid] = ' '.join(w for w in words)\n",
    "    captions_true[vid] = ' '.join(t for t in trues)\n",
    "    print(vid)\n",
    "    print('Pred Caption: ', ' '.join(w for w in words))\n",
    "    print('True Caption: ', ' '.join(t for t in trues))\n",
    "    print('-'*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vid = 2989.0\n",
    "\n",
    "num_frames = 10\n",
    "plt.figure(figsize=(200,20))\n",
    "for j in range(num_frames):\n",
    "    plt.subplot(1, num_frames, j+1)\n",
    "    framePath = os.getcwd() + \"/datasets/frames/video\" + str(int(vid)) + \"/frame\" + str(j+1) + '.jpg'\n",
    "    image = mpimg.imread(framePath)\n",
    "    plt.imshow(image,aspect='auto')\n",
    "    plt.axis('off')\n",
    "plt.show()\n",
    "print('Pred Caption: ', captions_pred[vid])\n",
    "print('True Caption: ',captions_true[vid])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Test Prediction and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
