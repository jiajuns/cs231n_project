{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Activation, Dense, Flatten, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers.convolutional import (Conv2D, MaxPooling3D, Conv3D, MaxPooling2D)\n",
    "from keras.layers.normalization import (BatchNormalization)\n",
    "from keras.models import Sequential, load_model\n",
    "from keras import regularizers\n",
    "from keras.utils import to_categorical\n",
    "from PIL import Image\n",
    "from models import models\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "from PIL import Image\n",
    "import time\n",
    "import warnings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# model\n",
    "def c3d(num_classes, shape):\n",
    "    \n",
    "    \n",
    "    model = Sequential()\n",
    "    # conv_1\n",
    "    conv_1 = Conv3D(32, (7,5,5), input_shape=shape, strides=(1, 2, 2))\n",
    "    model.add(conv_1)\n",
    "    print(conv_1.output_shape)\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling3D(pool_size=(1, 2, 2), strides=(1, 2, 2)))\n",
    "    \n",
    "    # conv_2 \n",
    "    conv_2 = Conv3D(64, (3,3,3))\n",
    "    model.add(conv_2)\n",
    "    print(conv_2.output_shape)\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling3D(pool_size=(1, 2, 2), strides=(1, 2, 2)))\n",
    "    \n",
    "    # conv_3\n",
    "    conv_3 = Conv3D(128, (2,2,2))\n",
    "    model.add(conv_3)\n",
    "    print(conv_3.output_shape)\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling3D(pool_size=(1, 2, 2), strides=(1, 2, 2)))\n",
    "\n",
    "\n",
    "    #\n",
    "    model.add(Flatten())\n",
    "    d1 = Dense(256)\n",
    "    model.add(d1)\n",
    "    print (d1.output_shape)\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    d2 = Dense(256)\n",
    "    model.add(d2)\n",
    "    print (d2.output_shape)\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    d3 = Dense(num_classes, activation='softmax')\n",
    "    model.add(d3)\n",
    "    print (d3.output_shape)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# process input data\n",
    "def dataPreProcess(x_train_ind, num_videos, frame_dir, shape):\n",
    "    tic = time.time()\n",
    "    \n",
    "    # create input data matrix\n",
    "    if len(shape) == 4:\n",
    "        num_frames, h, w, c = shape\n",
    "        data = np.zeros( (num_videos, num_frames, h, w, c))\n",
    "    else:\n",
    "        h, w, c = shape\n",
    "        data = np.zeros( (num_videos, h, w, c) )\n",
    "    \n",
    "    # read videos frames\n",
    "    count_video = 0    \n",
    "    for video_ind in x_train_ind:\n",
    "        # check video frames available\n",
    "        path = frame_dir +'/video'+str(video_ind)\n",
    "        if not os.path.exists(path): continue\n",
    "            \n",
    "        # read frames\n",
    "        count_video += 1\n",
    "        for fi in range(1,num_frames+1):\n",
    "            frame = Image.open(path+'/frame'+str(fi)+'.jpg')\n",
    "            frame = frame.resize( (h,w), Image.ANTIALIAS)\n",
    "            frame = np.asarray( frame, dtype=\"int32\" ) # transform to array\n",
    "            frame.flags.writeable = True\n",
    "            frame = frame.astype(np.float64)\n",
    "            # im_arr = preprocess_input(im_arr)   substraction, normaliztion ....\n",
    "            data[count_video-1, fi-1] = frame\n",
    "        \n",
    "        if count_video%250 == 0:\n",
    "            toc = time.time()-tic\n",
    "            print (\"finished %d video; elapsed time:%d s\" %(count_video toc) ) \n",
    "        \n",
    "        if count_video == num_videos:\n",
    "            print (\"finished all processing\")\n",
    "            break\n",
    "        \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished 250 video frames from 6783 videos, elapsed time:19 s\n",
      "finished 500 video frames from 3178 videos, elapsed time:43 s\n",
      "finished 750 video frames from 6213 videos, elapsed time:64 s\n",
      "finished 1000 video frames from 6233 videos, elapsed time:86 s\n",
      "finished 1250 video frames from 5760 videos, elapsed time:108 s\n",
      "finished 1500 video frames from 5440 videos, elapsed time:129 s\n",
      "finished 1750 video frames from 5364 videos, elapsed time:151 s\n",
      "finished 2000 video frames from 2768 videos, elapsed time:172 s\n",
      "finished 2250 video frames from 2778 videos, elapsed time:193 s\n",
      "finished 2500 video frames from 7154 videos, elapsed time:214 s\n",
      "finished all processing\n"
     ]
    }
   ],
   "source": [
    "# parameters setting\n",
    "num_frames = 10\n",
    "h, w, c = 64, 64, 3\n",
    "shape = (num_frames, h, w, c)\n",
    "num_videos = 2500\n",
    "curr_path = os.getcwd()\n",
    "frame_dir = curr_path + '/datasets/frames'\n",
    "\n",
    "x_train_ind = np.load(curr_path+'/datasets/train_ind_above400.npy')\n",
    "x_test_ind = np.load(curr_path+'/datasets/test_ind_above400.npy')\n",
    "\n",
    "\n",
    "y_train = np.load(curr_path+'/datasets/y_train_mapped_above400.npy')\n",
    "y_test = np.load(curr_path+'/datasets/y_test_mapped_above400.npy')\n",
    "num_classes = len(np.unique(y_train))\n",
    "\n",
    "\n",
    "if num_videos > len(x_train_ind):\n",
    "    warnings.warn(\"Exceed training data szie\", DeprecationWarning)\n",
    "    num_videos = len(x_train_ind)\n",
    "y_train = y_train[:num_videos]\n",
    "    \n",
    "    \n",
    "x_train = dataPreProcess(x_train_ind, num_videos, frame_dir, shape )\n",
    "y_train = to_categorical(y_train, num_classes=num_classes)\n",
    "\n",
    "# x_test = ataPreProcess(x_test_ind, num_videos, frame_dir, shape )\n",
    "# y_test = to_categorical(y_test, num_classes=num_classes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4257 4257 10\n",
      "(4257, 10, 64, 64, 3) (4257, 10)\n"
     ]
    }
   ],
   "source": [
    "print( len(x_train), len(y_train),  num_classes)\n",
    "print( x_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 64, 64, 3)\n",
      "(None, 4, 30, 30, 32)\n",
      "(None, 2, 13, 13, 64)\n",
      "(None, 1, 5, 5, 128)\n",
      "(None, 256)\n",
      "(None, 256)\n",
      "(None, 10)\n",
      "Train on 80 samples, validate on 20 samples\n",
      "Epoch 1/30\n",
      "80/80 [==============================] - 0s - loss: 29.5359 - acc: 0.5875 - val_loss: 26.8913 - val_acc: 1.0000\n",
      "Epoch 2/30\n",
      "80/80 [==============================] - 0s - loss: 26.6009 - acc: 1.0000 - val_loss: 25.7502 - val_acc: 1.0000\n",
      "Epoch 3/30\n",
      "80/80 [==============================] - 0s - loss: 25.4160 - acc: 1.0000 - val_loss: 24.4777 - val_acc: 1.0000\n",
      "Epoch 4/30\n",
      "80/80 [==============================] - 0s - loss: 24.1276 - acc: 1.0000 - val_loss: 23.1594 - val_acc: 1.0000\n",
      "Epoch 5/30\n",
      "80/80 [==============================] - 0s - loss: 22.8060 - acc: 1.0000 - val_loss: 21.8364 - val_acc: 1.0000\n",
      "Epoch 6/30\n",
      "80/80 [==============================] - 0s - loss: 21.4869 - acc: 1.0000 - val_loss: 20.5328 - val_acc: 1.0000\n",
      "Epoch 7/30\n",
      "80/80 [==============================] - 0s - loss: 20.1917 - acc: 1.0000 - val_loss: 19.2638 - val_acc: 1.0000\n",
      "Epoch 8/30\n",
      "80/80 [==============================] - 0s - loss: 18.9340 - acc: 1.0000 - val_loss: 18.0392 - val_acc: 1.0000\n",
      "Epoch 9/30\n",
      "80/80 [==============================] - 0s - loss: 17.7227 - acc: 1.0000 - val_loss: 16.8654 - val_acc: 1.0000\n",
      "Epoch 10/30\n",
      "80/80 [==============================] - 0s - loss: 16.5632 - acc: 1.0000 - val_loss: 15.7461 - val_acc: 1.0000\n",
      "Epoch 11/30\n",
      "80/80 [==============================] - 0s - loss: 15.4589 - acc: 1.0000 - val_loss: 14.6834 - val_acc: 1.0000\n",
      "Epoch 12/30\n",
      "80/80 [==============================] - 0s - loss: 14.4115 - acc: 1.0000 - val_loss: 13.6781 - val_acc: 1.0000\n",
      "Epoch 13/30\n",
      "80/80 [==============================] - 0s - loss: 13.4214 - acc: 1.0000 - val_loss: 12.7297 - val_acc: 1.0000\n",
      "Epoch 14/30\n",
      "80/80 [==============================] - 0s - loss: 12.4881 - acc: 1.0000 - val_loss: 11.8374 - val_acc: 1.0000\n",
      "Epoch 15/30\n",
      "80/80 [==============================] - 0s - loss: 11.6104 - acc: 1.0000 - val_loss: 10.9996 - val_acc: 1.0000\n",
      "Epoch 16/30\n",
      "80/80 [==============================] - 0s - loss: 10.7868 - acc: 1.0000 - val_loss: 10.2145 - val_acc: 1.0000\n",
      "Epoch 17/30\n",
      "80/80 [==============================] - 0s - loss: 10.0154 - acc: 1.0000 - val_loss: 9.4800 - val_acc: 1.0000\n",
      "Epoch 18/30\n",
      "80/80 [==============================] - 0s - loss: 9.2938 - acc: 1.0000 - val_loss: 8.7937 - val_acc: 1.0000\n",
      "Epoch 19/30\n",
      "80/80 [==============================] - 0s - loss: 8.6200 - acc: 1.0000 - val_loss: 8.1534 - val_acc: 1.0000\n",
      "Epoch 20/30\n",
      "80/80 [==============================] - 0s - loss: 7.9914 - acc: 1.0000 - val_loss: 7.5566 - val_acc: 1.0000\n",
      "Epoch 21/30\n",
      "80/80 [==============================] - 0s - loss: 7.4057 - acc: 1.0000 - val_loss: 7.0009 - val_acc: 1.0000\n",
      "Epoch 22/30\n",
      "80/80 [==============================] - 0s - loss: 6.8605 - acc: 1.0000 - val_loss: 6.4840 - val_acc: 1.0000\n",
      "Epoch 23/30\n",
      "80/80 [==============================] - 0s - loss: 6.3535 - acc: 1.0000 - val_loss: 6.0036 - val_acc: 1.0000\n",
      "Epoch 24/30\n",
      "80/80 [==============================] - 0s - loss: 5.8824 - acc: 1.0000 - val_loss: 5.5574 - val_acc: 1.0000\n",
      "Epoch 25/30\n",
      "80/80 [==============================] - 0s - loss: 5.4449 - acc: 1.0000 - val_loss: 5.1434 - val_acc: 1.0000\n",
      "Epoch 26/30\n",
      "80/80 [==============================] - 0s - loss: 5.0390 - acc: 1.0000 - val_loss: 4.7594 - val_acc: 1.0000\n",
      "Epoch 27/30\n",
      "80/80 [==============================] - 0s - loss: 4.6627 - acc: 1.0000 - val_loss: 4.4035 - val_acc: 1.0000\n",
      "Epoch 28/30\n",
      "80/80 [==============================] - 0s - loss: 4.3140 - acc: 1.0000 - val_loss: 4.0739 - val_acc: 1.0000\n",
      "Epoch 29/30\n",
      "80/80 [==============================] - 0s - loss: 3.9910 - acc: 1.0000 - val_loss: 3.7688 - val_acc: 1.0000\n",
      "Epoch 30/30\n",
      "80/80 [==============================] - 0s - loss: 3.6920 - acc: 1.0000 - val_loss: 3.4864 - val_acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "model = models(num_classes, \"c3d\", num_frames = 10, reg = 1e-1)\n",
    "history = model.model.fit(x_train[:500], y_train[:500], epochs=30, batch_size=32, validation_split = 0.2, verbose = 1)\n",
    "\n",
    "\n",
    "# model = c3d(num_classes, (num_frames, h, w, c))\n",
    "# model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# history = model.fit(x, y, epochs=30, batch_size=32, validation_split = 0.2, verbose = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot training history\n",
    "# print(history.history.keys())\n",
    "# plt.subplots(121)\n",
    "plt.plot(history.history['acc'])\n",
    "# # plt.plot(history.history['val_acc'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
